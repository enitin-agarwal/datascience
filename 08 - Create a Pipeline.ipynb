{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 파이프라인 만들기\r\n",
        "\r\n",
        "Azure ML SDK를 사용해 스크립트 기반 실험을 실행하면 데이터를 수집하고 모델을 학습시킨 다음 개별적으로 등록하는 데 필요한 여러 단계를 수행할 수 있습니다. 그러나 엔터프라이즈 환경에서는 보통 기계 학습 솔루션을 빌드하려면 수행해야 하는 개별 단계 순서를 *파이프라인*에 캡슐화합니다. 이 파이프라인은 사용자의 요청 시 컴퓨팅 대상 하나 이상에서 실행하거나, 자동화된 빌드 프로세스에서 실행하거나, 일정에 따라 실행할 수 있습니다.\r\n",
        "\r\n",
        "이 Notebook에서는 이러한 모든 요소를 취합하여 데이터를 전처리한 다음 모델 학습과 등록을 진행하는 간단한 파이프라인을 만들어 보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 작업 영역에 연결\r\n",
        "\r\n",
        "이 Notebook의 작업을 시작하려면 먼저 작업 영역에 연결합니다.\r\n",
        "\r\n",
        "> **참고**: Azure 구독에 인증된 세션을 아직 설정하지 않은 경우에는 링크를 클릭하고 인증 코드를 입력한 다음 Azure에 로그인하여 인증하라는 메시지가 표시됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import azureml.core\n",
        "from azureml.core import Workspace\n",
        "\n",
        "# 저장된 구성 파일에서 작업 영역 로드\r\n",
        "ws = Workspace.from_config()\n",
        "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 데이터 준비\r\n",
        "\r\n",
        "파이프라인에서는 당뇨병 환자의 세부 정보가 포함된 데이터 세트를 사용합니다. 아래 셀의 명령을 실행하여 이 데이터 세트를 만듭니다. 이전에 데이터 세트를 만든 경우, 코드가 기존 버전을 찾습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core import Dataset\n",
        "\n",
        "default_ds = ws.get_default_datastore()\n",
        "\n",
        "if 'diabetes dataset' not in ws.datasets:\n",
        "    default_ds.upload_files(files=['./data/diabetes.csv', './data/diabetes2.csv'], # Upload the diabetes csv files in /data\n",
        "                        target_path='diabetes-data/', # Put it in a folder path in the datastore\n",
        "                        overwrite=True, # Replace existing files of the same name\n",
        "                        show_progress=True)\n",
        "\n",
        "    #Create a tabular dataset from the path on the datastore (this may take a short while)\n",
        "    tab_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'diabetes-data/*.csv'))\n",
        "\n",
        "    # Register the tabular dataset\n",
        "    try:\n",
        "        tab_data_set = tab_data_set.register(workspace=ws, \n",
        "                                name='diabetes dataset',\n",
        "                                description='diabetes data',\n",
        "                                tags = {'format':'CSV'},\n",
        "                                create_new_version=True)\n",
        "        print('Dataset registered.')\n",
        "    except Exception as ex:\n",
        "        print(ex)\n",
        "else:\n",
        "    print('Dataset already registered.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 파이프라인 단계용 스크립트 만들기\r\n",
        "\r\n",
        "파이프라인은 *단계* 하나 이상으로 구성됩니다. 이러한 단계는 Python 스크립트일 수도 있고, 특정 위치 간에 데이터를 복사하는 데이터 전송 단계 등의 특수 단계일 수도 있습니다. 각 단계는 자체 컴퓨팅 컨텍스트에서 실행할 수 있습니다. 이 연습에서는 Python 스크립트 단계 2개가 포함된 간단한 파이프라인을 작성합니다. 한 단계에서는 학습 데이터를 전처리하며, 다른 단계에서는 전처리된 데이터를 사용하여 모델 학습과 등록을 진행합니다.\r\n",
        "\r\n",
        "먼저 파이프라인 단계에서 사용할 스크립트 파일용 폴더를 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "# 파이프라인 단계 파일용 폴더 만들기\r\n",
        "experiment_folder = 'diabetes_pipeline'\n",
        "os.makedirs(experiment_folder, exist_ok=True)\n",
        "\n",
        "print(experiment_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "이제 첫 번째 스크립트를 작성합니다. 이 스크립트는 당뇨병 데이터 세트의 데이터를 읽은 후 간단한 전처리 작업을 적용해 데이터가 누락된 행을 제거하고 숫자 특징을 크기가 비슷하도록 정규화합니다.\r\n",
        "\r\n",
        "이 스크립트에 포함된 **--prepped-data** 인수는 결과 데이터를 저장해야 하는 폴더를 참조합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $experiment_folder/prep_diabetes.py\n",
        "# 라이브러리 가져오기\r\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "from azureml.core import Run\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# 매개 변수 가져오기\r\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--input-data\", type=str, dest='raw_dataset_id', help='raw dataset')\n",
        "parser.add_argument('--prepped-data', type=str, dest='prepped_data', default='prepped_data', help='Folder for results')\n",
        "args = parser.parse_args()\n",
        "save_folder = args.prepped_data\n",
        "\n",
        "# 실험 실행 컨텍스트 가져오기\r\n",
        "run = Run.get_context()\n",
        "\n",
        "# 데이터 로드(입력 데이터 세트로 전달됨)\r\n",
        "print(\"Loading Data...\")\n",
        "diabetes = run.input_datasets['raw_data'].to_pandas_dataframe()\n",
        "\n",
        "# 원시 행 수 로깅\r\n",
        "row_count = (len(diabetes))\n",
        "run.log('raw_rows', row_count)\n",
        "\n",
        "# Null 제거\r\n",
        "diabetes = diabetes.dropna()\n",
        "\n",
        "# 숫자 열 정규화\r\n",
        "scaler = MinMaxScaler()\n",
        "num_cols = ['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree']\n",
        "diabetes[num_cols] = scaler.fit_transform(diabetes[num_cols])\n",
        "\n",
        "# 처리된 행 로깅\r\n",
        "row_count = (len(diabetes))\n",
        "run.log('processed_rows', row_count)\n",
        "\n",
        "# 준비된 데이터 저장\r\n",
        "print(\"Saving Data...\")\n",
        "os.makedirs(save_folder, exist_ok=True)\n",
        "save_path = os.path.join(save_folder,'data.csv')\n",
        "diabetes.to_csv(save_path, index=False, header=True)\n",
        "\n",
        "# 실행 종료\r\n",
        "run.complete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "이제 두 번째 단계(모델 학습)용 스크립트를 만들 수 있습니다. 이 스크립트에 포함된 **--training-data** 인수는 이전 단계에서 준비한 데이터가 저장된 위치를 참조합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $experiment_folder/train_diabetes.py\n",
        "# 라이브러리 가져오기\r\n",
        "from azureml.core import Run, Model\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 매개 변수 가져오기\r\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--training-data\", type=str, dest='training_data', help='training data')\n",
        "args = parser.parse_args()\n",
        "training_data = args.training_data\n",
        "\n",
        "# 실험 실행 컨텍스트 가져오기\r\n",
        "run = Run.get_context()\n",
        "\n",
        "# 학습 폴더에 준비된 데이터 파일 로드\r\n",
        "print(\"Loading Data...\")\n",
        "file_path = os.path.join(training_data,'data.csv')\n",
        "diabetes = pd.read_csv(file_path)\n",
        "\n",
        "# 기능 및 레이블 분리\r\n",
        "X, y = diabetes[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, diabetes['Diabetic'].values\n",
        "\n",
        "# 데이터를 학습 세트와 테스트 세트로 분할\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
        "\n",
        "# 의사 결정 트리 모델 학습 진행\r\n",
        "print('Training a decision tree model...')\n",
        "model = DecisionTreeClassifier().fit(X_train, y_train)\n",
        "\n",
        "# 정확도 계산\r\n",
        "y_hat = model.predict(X_test)\n",
        "acc = np.average(y_hat == y_test)\n",
        "print('Accuracy:', acc)\n",
        "run.log('Accuracy', np.float(acc))\n",
        "\n",
        "# AUC 계산\r\n",
        "y_scores = model.predict_proba(X_test)\n",
        "auc = roc_auc_score(y_test,y_scores[:,1])\n",
        "print('AUC: ' + str(auc))\n",
        "run.log('AUC', np.float(auc))\n",
        "\n",
        "# ROC 곡선 그리기\r\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
        "fig = plt.figure(figsize=(6, 4))\n",
        "# 대각선 50% 선 그리기\r\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "# 모델의 FPR 및 TPR 그리기\r\n",
        "plt.plot(fpr, tpr)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "run.log_image(name = \"ROC\", plot = fig)\n",
        "plt.show()\n",
        "\n",
        "# outputs 폴더에 학습된 모델 저장\r\n",
        "print(\"Saving model...\")\n",
        "os.makedirs('outputs', exist_ok=True)\n",
        "model_file = os.path.join('outputs', 'diabetes_model.pkl')\n",
        "joblib.dump(value=model, filename=model_file)\n",
        "\n",
        "# 모델 등록\r\n",
        "print('Registering model...')\n",
        "Model.register(workspace=run.experiment.workspace,\n",
        "               model_path = model_file,\n",
        "               model_name = 'diabetes_model',\n",
        "               tags={'Training context':'Pipeline'},\n",
        "               properties={'AUC': np.float(auc), 'Accuracy': np.float(acc)})\n",
        "\n",
        "\n",
        "run.complete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 파이프라인 단계용 컴퓨팅 환경 준비\r\n",
        "\r\n",
        "이 연습에서는 두 단계에 같은 컴퓨팅을 사용하지만 각 단계는 독립적으로 실행됩니다. 따라서 해당하는 경우 각 단계에 서로 다른 컴퓨팅 컨텍스트를 지정할 수 있습니다.\r\n",
        "\r\n",
        "먼저, 이전 랩에서 만든 컴퓨팅 대상을 가져옵니다. 없는 경우에는 생성됩니다.\r\n",
        "\r\n",
        "> **중요**: 컴퓨팅 클러스터를 실행하기 전에 아래 코드에서 *your-compute-cluster*를 컴퓨팅 클러스터의 이름으로 변경하세요! 클러스터 이름은 2~16자 사이의 전역으로 고유한 이름이어야 합니다. 유효한 문자는 영문자, 숫자 및 문자입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "cluster_name = \"your-compute-cluster\"\n",
        "\n",
        "try:\n",
        "    # Check for existing compute target\n",
        "    pipeline_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
        "    print('Found existing cluster, use it.')\n",
        "except ComputeTargetException:\n",
        "    # If it doesn't already exist, create it\n",
        "    try:\n",
        "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\n",
        "        pipeline_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
        "        pipeline_cluster.wait_for_completion(show_output=True)\n",
        "    except Exception as ex:\n",
        "        print(ex)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **참고**: 컴퓨팅 인스턴스와 클러스터는 표준 Azure 가상 머신 이미지를 기반으로 합니다. 이 연습에서는 비용과 성능 간 최적의 균형을 달성하기 위해 *Standard_DS11_v2* 이미지를 사용하는 것이 좋습니다. 구독의 할당량이 적어 이 이미지를 포함할 수 없는 경우 대체 이미지를 선택할 수 있습니다. 그러나 큰 이미지는 높은 비용을 야기할 수 있고 작은 이미지는 작업을 완료하는 데 충분하지 않을 수 있으므로 신중히 선택하는 것이 좋습니다. Azure 관리자에게 요청하여 할당량을 늘릴 수도 있습니다.\r\n",
        "\r\n",
        "컴퓨팅에는 필요한 패키지 종속성이 설치된 Python 환경이 필요합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $experiment_folder/experiment_env.yml\n",
        "name: experiment_env\n",
        "dependencies:\n",
        "- python=3.6.2\n",
        "- scikit-learn\n",
        "- ipykernel\n",
        "- matplotlib\n",
        "- pandas\n",
        "- pip\n",
        "- pip:\n",
        "  - azureml-defaults\n",
        "  - pyarrow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "이제 Conda 구성 파일이 있으므로 환경을 만들고 파이프라인의 실행 구성에서 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core import Environment\n",
        "from azureml.core.runconfig import RunConfiguration\n",
        "\n",
        "# 실험용 Python 환경 만들기(.yml 파일 사용)\r\n",
        "experiment_env = Environment.from_conda_specification(\"experiment_env\", experiment_folder + \"/experiment_env.yml\")\n",
        "\n",
        "# 환경 등록 \r\n",
        "experiment_env.register(workspace=ws)\n",
        "registered_env = Environment.get(ws, 'experiment_env')\n",
        "\n",
        "# 파이프라인용 새 runconfig 개체 만들기\r\n",
        "pipeline_run_config = RunConfiguration()\n",
        "\n",
        "# 위에서 만든 컴퓨팅을 사용합니다. \r\n",
        "pipeline_run_config.target = pipeline_cluster\n",
        "\n",
        "# 실행 구성에 환경 할당\r\n",
        "pipeline_run_config.environment = registered_env\n",
        "\n",
        "print (\"Run configuration created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 파이프라인 작성 및 실행\r\n",
        "\r\n",
        "이제 파이프라인을 만들고 실행할 수 있습니다.\r\n",
        "\r\n",
        "먼저 파이프라인용 단계와 단계 간에 전달해야 하는 데이터 참조를 정의해야 합니다. 이 연습의 첫 번째 단계는 두 번째 단계에서 읽을 수 있는 폴더에 준비된 데이터를 써야 합니다. 이 두 단계는 원격 컴퓨팅에서 실행되며 각기 다른 컴퓨팅에서 실행할 수 있으므로, 작업 영역 내 데이터 저장소의 특정 위치에 대한 데이터 참조로 폴더 경로를 전달해야 합니다. **OutputFileDatasetConfig** 개체는 중간 스토리지 위치에 사용되며 파이프라인 단계 간에 전달할 수 있는 특수한 종류의 데이터 참조입니다. 여기서는 PipelineData 개체를 만들어 첫 번째 단계의 출력/두 번째 단계의 입력으로 사용할 것입니다. 또한 데이터 참조를 통해 참조하는 데이터 저장소 위치에 코드가 액세스할 수 있도록 이 개체를 스크립트 인수로 전달해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.data import OutputFileDatasetConfig\n",
        "from azureml.pipeline.steps import PythonScriptStep\n",
        "\n",
        "# 학습 데이터 세트 가져오기\r\n",
        "diabetes_ds = ws.datasets.get(\"diabetes dataset\")\n",
        "\n",
        "# 1단계에서 2단계로 전달된 데이터를 보관할 OutputFileDatasetConfig(임시 데이터 참조)를 만듭니다.\r\n",
        "prepped_data = OutputFileDatasetConfig(\"prepped_data\")\n",
        "\n",
        "# 1단계: 데이터 준비 스크립트 실행\r\n",
        "prep_step = PythonScriptStep(name = \"Prepare Data\",\n",
        "                                source_directory = experiment_folder,\n",
        "                                script_name = \"prep_diabetes.py\",\n",
        "                                arguments = ['--input-data', diabetes_ds.as_named_input('raw_data'),\n",
        "                                             '--prepped-data', prepped_data],\n",
        "                                compute_target = pipeline_cluster,\n",
        "                                runconfig = pipeline_run_config,\n",
        "                                allow_reuse = True)\n",
        "\n",
        "# 2단계: 학습 스크립트 실행\r\n",
        "train_step = PythonScriptStep(name = \"Train and Register Model\",\n",
        "                                source_directory = experiment_folder,\n",
        "                                script_name = \"train_diabetes.py\",\n",
        "                                arguments = ['--training-data', prepped_data.as_input()],\n",
        "                                compute_target = pipeline_cluster,\n",
        "                                runconfig = pipeline_run_config,\n",
        "                                allow_reuse = True)\n",
        "\n",
        "print(\"Pipeline steps defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "이제 정의한 단계에서 파이프라인을 작성하여 실험으로 실행할 준비가 되었습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "from azureml.core import Experiment\n",
        "from azureml.pipeline.core import Pipeline\n",
        "from azureml.widgets import RunDetails\n",
        "\n",
        "# 파이프라인 생성\r\n",
        "pipeline_steps = [prep_step, train_step]\n",
        "pipeline = Pipeline(workspace=ws, steps=pipeline_steps)\n",
        "print(\"Pipeline is built.\")\n",
        "\n",
        "# 실험 작성 및 파이프라인 실행\r\n",
        "experiment = Experiment(workspace=ws, name = 'mslearn-diabetes-pipeline')\n",
        "pipeline_run = experiment.submit(pipeline, regenerate_outputs=True)\n",
        "print(\"Pipeline submitted for execution.\")\n",
        "RunDetails(pipeline_run).show()\n",
        "pipeline_run.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "파이프라인이 실행되면 그래픽 형식 파이프라인 실험이 위젯에 표시됩니다. 페이지 오른쪽 위에 있는 커널 표시기를 주의 깊게 살펴보세요. **&#9899;**에서 **&#9711;**로 바뀌면 코드 실행이 완료된 것입니다. [Azure Machine Learning Studio](https://ml.azure.com)의 **실험** 페이지에서 파이프라인 실행을 모니터링할 수도 있습니다.\r\n",
        "\r\n",
        "파이프라인이 완료되고 나면 하위 실행에서 기록된 메트릭을 검사할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for run in pipeline_run.get_children():\n",
        "    print(run.name, ':')\n",
        "    metrics = run.get_metrics()\n",
        "    for metric_name in metrics:\n",
        "        print('\\t',metric_name, \":\", metrics[metric_name])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "파이프라인 실행이 정상적으로 완료되면 *학습 컨텍스트* 태그가 지정된 새 모델이 등록됩니다. 이 태그는 해당 모델이 파이프라인에서 학습되었음을 나타냅니다. 다음 코드를 사용하여 모델 등록 여부를 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core import Model\n",
        "\n",
        "for model in Model.list(ws):\n",
        "    print(model.name, 'version:', model.version)\n",
        "    for tag_name in model.tags:\n",
        "        tag = model.tags[tag_name]\n",
        "        print ('\\t',tag_name, ':', tag)\n",
        "    for prop_name in model.properties:\n",
        "        prop = model.properties[prop_name]\n",
        "        print ('\\t',prop_name, ':', prop)\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 파이프라인 게시\r\n",
        "\r\n",
        "작성하여 테스트한 파이프라인은 REST 서비스로 게시할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 실행에서 파이프라인 게시\r\n",
        "published_pipeline = pipeline_run.publish_pipeline(\n",
        "    name=\"diabetes-training-pipeline\", description=\"Trains diabetes model\", version=\"1.0\")\n",
        "\n",
        "published_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "게시된 파이프라인에는 엔드포인트가 있습니다. [Azure Machine Learning Studio](https://ml.azure.com)의 (**파이프라인 엔드포인트** 탭에 있는) **엔드포인트** 페이지에서 확인할 수 있습니다. 게시된 파이프라인 개체의 속성으로 엔드포인트의 URI를 확인할 수도 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rest_endpoint = published_pipeline.endpoint\n",
        "print(rest_endpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 파이프라인 엔드포인트 호출\r\n",
        "\r\n",
        "엔드포인트를 사용하려면 클라이언트 애플리케이션이 HTTP를 통해 REST 호출을 수행해야 합니다. 이 요청은 인증해야 하므로 인증 헤더가 필요합니다. 실제 애플리케이션에는 인증에 사용할 서비스 주체가 필요합니다. 여기서는 이 인증 과정을 테스트하기 위해 현재 Azure 작업 영역에 설정되어 있는 연결의 인증 헤더를 사용합니다. 다음 코드를 사용하면 해당 인증 헤더를 가져올 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core.authentication import InteractiveLoginAuthentication\n",
        "\n",
        "interactive_auth = InteractiveLoginAuthentication()\n",
        "auth_header = interactive_auth.get_authentication_header()\n",
        "print(\"Authentication header ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "이제 REST 인터페이스를 호출할 수 있습니다. 파이프라인은 비동기식으로 실행되므로 식별자를 다시 가져와야 합니다. 이 식별자는 실행 중인 파이프라인 실험을 추적하는 데 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "experiment_name = 'mslearn-diabetes-pipeline'\n",
        "\n",
        "rest_endpoint = published_pipeline.endpoint\n",
        "response = requests.post(rest_endpoint, \n",
        "                         headers=auth_header, \n",
        "                         json={\"ExperimentName\": experiment_name})\n",
        "run_id = response.json()[\"Id\"]\n",
        "run_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "실행 ID가 있으므로 이를 사용하여 실행이 완료될 때까지 기다릴 수 있습니다.\r\n",
        "\r\n",
        "> **참고**: 각 단계는 출력을 재사용할 수 있도록 구성되었으므로 파이프라인 실행은 빠르게 완료됩니다. 이 구성은 기본적으로 과정 진행 시간을 절약하기 위해 편의상 적용된 것입니다. 실제로는 데이터가 변경될 때마다 첫 단계를 실행하고 1단계의 출력이 변경되는 경우에만 후속 단계를 트리거할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.pipeline.core.run import PipelineRun\n",
        "\n",
        "published_pipeline_run = PipelineRun(ws.experiments[experiment_name], run_id)\n",
        "published_pipeline_run.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 파이프라인 예약\r\n",
        "\r\n",
        "당뇨병 환자를 진료하는 병원에서 매주 새 데이터를 수집하여 데이터 세트에 추가한다고 가정해 보겠습니다. 이 경우 매주 파이프라인을 실행하여 새 데이터로 모델을 다시 학습시킬 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.pipeline.core import ScheduleRecurrence, Schedule\n",
        "\n",
        "# 매주 월요일 00:00 UTC에 파이프라인 제출\r\n",
        "recurrence = ScheduleRecurrence(frequency=\"Week\", interval=1, week_days=[\"Monday\"], time_of_day=\"00:00\")\n",
        "weekly_schedule = Schedule.create(ws, name=\"weekly-diabetes-training\", \n",
        "                                  description=\"Based on time\",\n",
        "                                  pipeline_id=published_pipeline.id, \n",
        "                                  experiment_name='mslearn-diabetes-pipeline', \n",
        "                                  recurrence=recurrence)\n",
        "print('Pipeline scheduled.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "다음과 같이 작업 영역에 정의된 일정을 검색할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "schedules = Schedule.list(ws)\n",
        "schedules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "다음과 같이 최신 실행을 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_experiment = ws.experiments.get('mslearn-diabetes-pipeline')\n",
        "latest_run = list(pipeline_experiment.get_runs())[0]\n",
        "\n",
        "latest_run.get_details()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "이 연습은 파이프라인 작성 원칙을 보여 주는 간단한 예제입니다. 실제로는 더 복잡한 논리를 작성하여 파이프라인 단계에 포함할 수 있습니다. 예를 들어 특정 테스트 데이터를 기준으로 모델을 평가해 AUC, 정확도 등의 성능 메트릭을 계산하고, 이 메트릭을 이전에 등록한 모델 버전의 메트릭과 비교한 다음 성능이 더 우수한 경우에만 새 모델을 등록할 수 있습니다.\r\n",
        "\r\n",
        "[Azure DevOps용 Azure Machine Learning 확장](https://marketplace.visualstudio.com/items?itemName=ms-air-aiagility.vss-services-azureml)을 사용하여 Azure ML 파이프라인을 Azure DevOps 파이프라인과 결합한 다음 *CI/CD(연속 통합/연속 배포)* 프로세스에 모델 재학습 과정을 통합할 수 있습니다. 이 두 파이프라인은 이름이 같아* *혼동할 수 있으므로 주의하세요. 예를 들어 Azure DevOps *빌드* 파이프라인을 사용해 모델 학습과 등록을 수행하는 Azure ML 파이프라인을 트리거할 수 있습니다. 모델이 등록되면 빌드 파이프라인은 Azure DevOps *릴리스* 파이프라인을 트리거할 수 있습니다. 릴리스 파이프라인은 모델을 사용하는 애플리케이션이나 서비스와 함께 모델을 웹 서비스로 배포합니다."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.6 - AzureML",
      "language": "python",
      "name": "python3-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}